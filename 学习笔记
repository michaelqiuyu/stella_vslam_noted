1. 编译运行：
	1.1 按照https://stella-cv.readthedocs.io/en/latest/installation.html#subsection-common-linux-macos安装相关以来
	1.2 编译安装FBOW
	1.3 编译安装tinycolormap
	1.4 编译安装filesystem
	1.5 编译的可执行文件在build目录下
		1.5.1 mkdir build && cd build
		1.5.2 cmake \
		        -DCMAKE_BUILD_TYPE=Release \
			    -DUSE_PANGOLIN_VIEWER=ON \
			    -DINSTALL_PANGOLIN_VIEWER=ON \
			    -DUSE_SOCKET_PUBLISHER=OFF \
			    -DBUILD_TESTS=ON \
			    -DBUILD_EXAMPLES=ON \
			    ..
		1.5.3 make -j8
	1.6 词袋下载地址：
		1.6.1 https://github.com/stella-cv/FBoW_orb_vocab
		1.6.2 curl -sL "https://github.com/stella-cv/FBoW_orb_vocab/raw/main/orb_vocab.fbow" -o orb_vocab.fbow
	1.7 https://stella-cv.readthedocs.io/en/latest/simple_tutorial.html：在此目录下下载数据并运行
	1.8 example:
		1.8.1 create_map
			1. 1280 * 640
				1.1 ./run_video_slam \
			    -v /home/xiongchao/workspace/leador/project/vslam/code/stella_vslam_noted/3rd/FBoW/orb_vocab.fbow \
			    -c /home/xiongchao/workspace/leador/project/vslam/dataset/leador/beijing_leador/1280_640/instal360.yaml \
			    -m /home/xiongchao/workspace/leador/project/vslam/dataset/leador/beijing_leador/1280_640/parking_3min.mp4 \
			    --map-db-out /home/xiongchao/workspace/leador/project/vslam/dataset/leador/beijing_leador/1280_640/parking_3min.msg
			2 1920 * 960
				2.1 ./run_video_slam \
			    -v /home/xiongchao/workspace/leador/project/vslam/code/stella_vslam_noted/3rd/FBoW/orb_vocab.fbow \
			    -c /home/xiongchao/workspace/leador/project/vslam/dataset/leador/beijing_leador/1920_960/instal360.yaml \
			    -m /home/xiongchao/workspace/leador/project/vslam/dataset/leador/beijing_leador/1920_960/beijing_leador_company.mp4 \
			    --map-db-out /home/xiongchao/workspace/leador/project/vslam/dataset/leador/beijing_leador/1920_960/beijing_leador_company.msg



		
		1.8.2 run localization
			1. 1920 * 960
				1.1 定位模式
					./run_insta360 \
						--disable-mapping \
						-v /home/xiongchao/workspace/leador/project/vslam/code/stella_vslam_noted/3rd/FBoW/orb_vocab.fbow \
						-m /home/xiongchao/workspace/leador/project/vslam/dataset/leador/beijing_leador/1920_960/beijing_leador_company.mp4 \
						-c /home/xiongchao/workspace/leador/project/vslam/dataset/leador/beijing_leador/1920_960/instal360.yaml \
						--map-db-in /home/xiongchao/workspace/leador/project/vslam/dataset/leador/beijing_leador/1920_960/beijing_leador_company.msg
				1.2 逐帧重定位
					./run_insta360 \
						--disable-mapping \
						-v /home/xiongchao/workspace/leador/project/vslam/code/stella_vslam_noted/3rd/FBoW/orb_vocab.fbow \
						-m /home/xiongchao/workspace/leador/project/vslam/dataset/leador/beijing_leador/1920_960/beijing_leador_company.mp4 \
						-c /home/xiongchao/workspace/leador/project/vslam/dataset/leador/beijing_leador/1920_960/instal360.yaml \
						--frame-skip 50 \
						--is-reloc \
						--map-db-in /home/xiongchao/workspace/leador/project/vslam/dataset/leador/beijing_leador/1920_960/beijing_leador_company.msg

2. 









refine_pose_by_local_map函数中if (num_valid_obs > num_tracked_lms * ratio)判断有误





1. 传入图像、时间戳、mask图像
	1.1 创建单目frame：
		转换为灰度图 
		提取orb特征点并计算描述子
			1. 构建金字塔 
			2. 如果给定mask_rects那么将自动生成一个mask
			3. 提取fast角点：此处与ORB-SLAM2/3不同的是：提取的终止条件不一样，在ORB-SLAM2/3中，终止条件是叶子节点的数量达到指定值，而这里是分割的矩形的面积达到指定值；
				举例说明两者的不同：如果图像中仅有局部区域A、B、C和D中存在特征点，并且A和B中的特征点的数量明显远远多于C和D，并且所有区域的面积将小于给定阈值，那么VSLAM仅仅只会提取出4个角点，而按照ORB-SLAM2/3的方法将会在A和B上提取出远多于C和D的特征点，特征点的分布将向A和B靠近；从这个角度将，VSLAM的终止条件更加合理；但是当特征点在局部过分集中的时候也会使得VSLAM提取的特征点太少，有可能使得约束不够，从而导致退化，而此时ORB-SLAM2/3会提取出相当多的特征点，但是由于分布的很集中，也很难说这些约束不是过剩了，也就是说同样有可能退化
			4. 高斯模糊使得图像平滑，然后提取特征点
		对提取的特征点去畸变
		计算每一个去畸变的特征点对应的bearing vector
		将去畸变的特征点分配到网格中
		自动检测marker（系统已经实现了aruco检测器），保存检测到的marker信息用于后面的跟踪

		后面优化中使用的都是去畸变的点，对于鱼眼相机而言失去了大鱼眼的优势；并且由于去了畸变，因此对于透视相机和鱼眼相机的残差模型实际上是一样的（代码里面实现的代码一抹一样，这里的实现并不好，有大量的重复代码）

	1.2 双目frame
		提取左右目的特征点并计算描述子；对左图像的特征点去畸变，使用立体视觉的算法计算像素点对应的深度
		仅仅将左图像的去畸变的特征点对应的bearing vector；仅仅将左图像的去畸变的特征点分配到网格中
		仅仅对左图像执行marker检测

	1.3 RGBD相机自带深度，因此与双目相比，就是没有了立体视觉计算深度的步骤

	跟踪：
		初始化:
			1. 单目初始化：

				前后两帧匹配：对前一帧的每一个特征点，在后一帧的同一个金字塔层级上进行相同区域搜索，获取匹配
				
				由于equirectangular模型与perspective和fisheye的模型差别较大（也就是没有内参矩阵K）；因此equirectangular模型只能使用本质矩阵E来初始化，其余的相机模型通过几何自动选择使用F还是H来初始化

				附上H恢复的论文，E恢复的笔记

				根据计算得到的初始化的两帧以及匹配进行三角化

				创建关键帧，保存三维点和关键帧到地图，添加三维点的观测；保存marker点到地图并添加marker点的观测

				使用当前的关键帧和地图点以及marker点（相当于单目的2D-3D匹配），执行全局优化，对于双目而言残差多了一个视差的维度；

				计算所有的地图点的中值深度，并以此为单位1，重新构建地图


			2. 双目/RGBD初始化
				只要深度大于0的地图点的数量大于阈值就认为初始化成功
				创建关键帧，保存三维点和关键帧到地图，添加三维点的观测，没有添加marker的逻辑

			3. 如果初始化成功，将当前的关键帧插入到局部建图线程中

		重定位：
			1. 强制重定位：给与当前帧一个初始位姿用于搜索候选关键帧
			2. 重定位：根据词袋计算候选关键帧
			3. EPNP + RANSAC求解位姿，这里可以根据参数选择是使用词袋搜索还是直接使用brute-force暴力匹配：附上语雀上的EPNP文档
			4. EPNP求解成功后，根据2D-3D匹配进行位姿优化（固定地图点）；此处的优化执行多次，每次优化后根据结果，将残差大于阈值的边设置为不参与下一次优化；与ORB-SLAM3不同的是，这里下一次优化的初值是上一次优化的结果，而ORB-SLAM3每次都从最开始的初值开始优化（实际上是为了避免上一步中错误的边带来的误差）
			5. 利用投影匹配增加2D-3D匹配，然后再次固定地图点对位姿进行优化；此过程重复两次
			6. 通过当前帧的地图点的观测查找局部地图关键帧并获取局部关键帧的地图点从而构建局部地图；将局部地图中的地图点投影到当前帧构建匹配，然后再次固定特征点优化位姿

		跟踪：
			恒速模型跟踪：
				根据速度计算当前帧的位姿，然后利用位姿使用投影匹配将上一帧的地图点往当前帧投影获取2D-3D匹配（如果匹配数量不够就将搜索的范围扩大），然后固定地图点优化当前帧的位姿

			如果恒速模型失败，那么执行词袋匹配跟踪：
				使用当前帧和上一帧的参考关键帧计算词袋匹配，然后固定地图点优化当前帧的位姿，根据优化结果删除外点

			如果词袋匹配跟踪失败，那么执行鲁棒匹配跟踪：
				使用当前帧和上一帧的参考关键帧计算brute-force匹配，然后固定地图点优化当前真的位姿，根据优化结果删除外点

		重定位或者跟踪成功后会进行局部地图跟踪、计算恒速模型的速度
			局部建图跟踪：
				根据当前帧的地图点的观测获取局部关键帧，并获取局部关键帧的地图点，从而得到局部地图，然后将局部地图点往当前帧投影获取2D-3D匹配，然后固定地图点优化当前帧的位姿

		如果跟踪成功，那么会判断是否需要插入关键帧，如果需要就插入关键帧：
			插入关键帧的条件：
				如果才重定位不就，不插入关键帧，否则

				1. mapper_is_skipping_localBA：局部建图模块不可插入关键帧
				2. enough_keyfrms：当前地图中的关键帧的数量大于给定的阈值
				3. max_interval_elapsed：当前帧的时间戳减去上一关键帧的时间戳不小于最大时间间隔（1s）
				4. min_interval_elapsed：当前帧的时间戳减去上一关键帧的时间戳不小于最小时间间隔（1s）
				5. max_distance_traveled：当前帧与是上一关键帧的距离大于最大距离；代码中实际上并没有判断，取默认值false
				6. view_changed：当前帧跟踪的很好（也就是跟踪的地图点的观测的数量大于阈值）的地图点数量小于上一关键帧跟踪的很好的地图点的数量乘以一个指定的阈值（0.8）
				7. not_enough_lms：当前帧跟踪的很好的地图点的数量小于给定的阈值（15）
				8. tracking_is_unstable：当前帧跟踪的地图点的数量小于阈值
				9. 当前帧跟踪的很好（也就是跟踪的地图点的观测的数量大于阈值）的地图点数量大于上一关键帧跟踪的很好的地图点的数量乘以一个指定的阈值（0.9）

				表达式1：至少满足上面的3、5、6、7；也就是经历了较长时间（时间上），或者距离上一个关键帧已经很远（空间上），或者与上一关键帧相比跟踪的已经比较差了，或者本身就已经跟踪的很差了
				表达式2：2不满足或者4满足；关键帧的数量并不多或者已经超过可以插入关键帧的最小时间间隔了（最基本条件已经满足）
				表达式3：不满足8，也就是说跟踪的太差的时候，可能就是马上要跟丢了，这个时候不执行创建关键帧，而是去重定位才是合理的
				表达式4：不满足9，也就是跟踪的太好也没必要创建关键帧
				表达式5：不满足1，也就是局部建图模块可以插入关键帧

			插入关键帧：
				更新当前关键帧的观测：也就是更新当前关键帧看到的地图点的相关信息-添加地图点对这个关键帧的观测，重新计算方向和描述子
				将这个关键帧的marker信息写入到地图数据库中
				将这个关键帧送入大局部建图线程

				对于双目/RGBD，会增加一些深度小于阈值的地图点用于增强跟踪效果



局部建图线程：
	1. 获取关键帧队列的第一帧（也就是最老的那一帧），然后从队列中删除
	2. 存储关键帧的信息：将关键帧的观测存储到fresh_landmarks_中，更新共视图，将当前关键帧添加到地图数据库中
	3. 删除当前关键帧的地图点中的无效的地图点：从数据库中删除可观性不佳的地图点（跟踪到该MapPoint的Frame数相比预计可观测到该MapPoint的Frame数的比例小于阈值）；当然这里的删除有两种，一种是从fresh_landmarks_中删除，也就是这个点经过了检验，是一个还不错的地图点；另一种是从地图数据库中删除，这种点就是不好的点，不会参与以后的操作了；这里的逻辑要比ORB-SLAM2/3简单，ORB_SLAM2/3还会对地图点的观测的数量进行判断
	4. 创建新的地图点：
	




把ORB-SLAM2/3的关键帧创建的判断条件补上，并且说明区别


ORB-SLAM2/3中统一使用的是float，但是VSLAM中使用的是double；float运算更快，可以将VSLAM的double全部换为float


将VSLAM的stereo修改为仅仅只需要输入R, t即可，对于匹配相关的代码都需要将baseline删除


与ORB-SALM2/3相比，思路大致一样，但是阈值大多都不一样



ORB-SLAM2/3词袋检索（使用得分的时候）有bug，需要重新计算得分



论文阅读：
	1. 摘要

	2. 介绍

	3. 详细原理

	4. 测试








从debug的结果看出，这里提出的特征点其实是非常多的，对于1920 * 1080的图片来说，正常情况下提取了接近4000个特征点
但是从最后形成的地图点看，其数量又相比于ORB-SLAM3来说比较少，哪里的筛选条件比较严格，使得其效果变好





当前已经完成的功能：
	1. 支持逐帧重定位
	2. 支持通过宏命令，存储图像和灰度图像和加载图像和灰度图像：对msgpack的修改已经完成，对sqlite3的修改还没有进行



1. 添加如果是纯色则跳过
2. 读取图像的mat矩阵